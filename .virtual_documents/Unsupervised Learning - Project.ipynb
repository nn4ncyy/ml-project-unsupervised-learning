











import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. Data Import
df = pd.read_csv("Wholesale_Data.csv")

# Display first few rows
print("Dataset Head:")
print(df.head())


# 2. Data Cleaning

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Remove any negative values if present
df = df[(df >= 0).all(axis=1)]


# 3. Data Description

# Summary statistics
print("\nSummary Statistics:")
print(df.describe())


# 4. Data Visualization

sns.pairplot(df)
plt.show()

df.hist(figsize=(12, 10), bins=20, edgecolor='black')
plt.suptitle("Feature Distributions")
plt.show()


# 5. Outlier Detection

# Boxplot for outlier detection
plt.figure(figsize=(12, 6))
df.boxplot(rot=45)
plt.title("Boxplot for Outlier Detection Before")
plt.show()

# Log Transformation to reduce the impact of outliers
df_transformed = np.log1p(df)  # log1p ensures log(0) is handled safely
df = pd.DataFrame(df_transformed, columns=df.columns)

# Boxplot for outlier detection after we log transform
plt.figure(figsize=(12, 6))
df.boxplot(rot=45)
plt.title("Boxplot for Outlier Detection After")
plt.show()


# 6. Correlation Analysis

# Correlation Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

# Calculate the correlation coefficients between all pairs of variables
correlation_matrix = df.corr()

# Display the correlation matrix
print("\nCorrelation Coefficients between all variables:")
print(correlation_matrix)


# 7. Data Transformation
# Standardization
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)


# 8. Feature Selection

# PCA for Feature Selection
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)
print("\nExplained Variance Ratio:")
print(pca.explained_variance_ratio_)

# Scatter plot of first two principal components
plt.figure(figsize=(8, 6))
sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1])
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Analysis")
plt.show()








import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Select relevant columns for clustering
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
df_clustering = df[features]

# Standardize the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_clustering)


# Elbow Method to find optimal number of clusters
inertia = []
k_range = range(1, 11)  # Trying k from 1 to 10 clusters

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(k_range, inertia, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia (Sum of Squared Distances)")
plt.title("Elbow Method to Determine Optimal Number of Clusters")
plt.show()

# Apply KMeans clustering with optimal k
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(df_scaled)

# Get the cluster labels for each data point
labels = kmeans.labels_

# Add cluster labels to the original DataFrame
df['Cluster'] = labels

# PCA to reduce the data to 2D for visualization
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)

# Scatter plot of the first two principal components
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df_pca[:, 0], y=df_pca[:, 1], hue=df['Cluster'], palette="Set1", s=100, alpha=0.7)
plt.title("K-Means Clustering Visualization (PCA-reduced data)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title='Cluster', loc='upper right')
plt.show()










from sklearn.preprocessing import StandardScaler
import scipy.cluster.hierarchy as sch

# Select relevant columns for clustering
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
df_clustering = df[features]

# Standardize the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_clustering)

# Compute the distance matrix (using Euclidean distance by default)
distance_matrix = sch.distance.pdist(df_scaled, metric='euclidean')

plt.figure(figsize=(10, 6))
sch.dendrogram(sch.linkage(df_scaled, method='ward'))  # 'ward' minimizes variance within clusters
plt.title("Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show() 









from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Select the relevant columns for PCA (assuming these are the features to analyze)
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
df_clustering = df[features]

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_clustering)

# Apply PCA
pca = PCA()
pca.fit(df_scaled)

# Explained Variance Ratio
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratio for Each Principal Component:")
print(explained_variance_ratio)

# Transform the data to the first two principal components
df_pca = pca.transform(df_scaled)

# Scatter plot of the first two principal components
plt.figure(figsize=(8, 6))
plt.scatter(df_pca[:, 0], df_pca[:, 1], alpha=0.7, c='b', s=50)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA - Data projected onto the first two Principal Components")
plt.grid(True)
plt.show()










